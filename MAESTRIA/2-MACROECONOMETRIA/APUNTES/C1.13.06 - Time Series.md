
▶ Kilian & Lütkepohl, 2017. *Structural Vector Autoregressive Analysis*.  
▶ Hamilton, 1994. *Time Series Analysis*.  
▶ Lütkepohl, 2005. *New Introduction to Multiple Time Series Analysis*.  
▶ Pesaran, 2015. *Time Series and Panel Data Econometrics*.  
▶ Taylor & Uhlig (eds.), 2016. *Handbook of Macroeconomics*, Vol. 2.  
▶ Canova, 2007. *Methods for Applied Macroeconomic Research*.  
▶ Geweke, J. 2005, *Contemporary Bayesian Econometrics and Statistics*.  
▶ Koop, 2003, *Bayesian Econometrics*.
# Macroeconometría
▶ Nakamura & Steinsson, 2018. "Identification in Macroeconomics," *J. of Econ. Perspectives*, 32(3).  
▶ Stock & Watson. 2017. "Twenty Years of Time Series Econometrics in Ten Pictures," *J. of Econ. Perspectives*, 31(2).
▶ Diebold, 2017, *Forecasting in Economics, Business, Finance and Beyond*, Ch. 6-7.  
▶ Stock & Watson, 2020, *Introduction to Econometrics* (4th ed.), Ch. 15-17.

## Objetos de Estudio

▶ Variable(s) de interés: $y_t$. Conjunto de información en $t$: $\mathcal{I}_t$ (incluye $y_t, y_{t-1}, ...$)  
▶ **Pronósticos:**
▶ No restringidos:
   $$E (y_{T+h}|\mathcal{I}_{T-1}) , \text{ para } h = 0, 1, ..., H.$$
▶ Restringidos:
$$E (y_{T+h}|\mathcal{I}_{T-1}, x_T , x_{T+1}, ..., x_{T+J} ) , \text{ para } h = 0, 1, ..., H.$$
para alguna(s) variable(s) $x_t \in \mathcal{I}_t$.

▶ **Efecto Causal/Impulse Response/Análisis Estructural:**
$$E (y_{T+h}|\mathcal{I}_{T-1}, s_t = \bar{s}) - E (y_{T+h}|\mathcal{I}_{T-1}) , \text{ para } h = 0, 1, ..., H.$$
para alguna(s) variable(s) $s_t \in \mathcal{I}_t$. $s$ valor de interes, ej. que $\bar{s}$ sea la depreciacion del tipo de cambio objetivo. $E (y_{T+h}|\mathcal{I}_{T-1})$ es el comportamiento promedioque hubieras tenido sin $s_{t}=\bar{s}$. Si son lineales los modelos es todo lo mismo. Si no, no es tan obvio contra qué estas comparando.
Ejemplo: DSGE
## Enfoques principales

Dos enfoques principales:
1. Modelar el comportamiento dinámico completo de las variables relevantes ⇒ Calcular la expectativa condicional de interés.
2. Modelar la expectativa condicional de interés directamente.

**Pros y Contras:**
1. **Pros:** Se pueden hacer muchas cosas con un modelo (ej. pronóstico, efecto causal a diferentes tratamientos, etc.). **Contras:** Requiere imponer mucha estructura (supuestos), estimar muchos parámetros, y puede ser difícil de manejar computacionalmente (maldición de la dimensionalidad).
2. **Pros:** Requiere menos supuestos y parámetros; más fácil capturar algunas no linealidades. **Contras:** Útil para un solo objetivo, pero no todos; puede ocultar supuestos relevantes requeridos.
# Series de Tiempo

▶ Procesos Estacionarios  
▶ Regresiones con variables estacionarias.  
▶ No-estacionariedad y co-integración.  
▶ Comentarios sobre asintóticas en series temporales.

# Procesos estacionarios

## Estacionariedad

▶ Considere un proceso estocástico univariado $\{y_t\}$.  
▶ **Algunas definiciones:**
 ▶ Expectativa No Condicional: $E(y_t) = \mu_t$.  
 ▶ Varianza No Condicional: $V(y_t) = \gamma_{0,t}$.  
 ▶ Covarianza No Condicional: $Cov(y_t, y_{t-j}) = \gamma_{j,t}$.

▶ Se dice que un proceso es **(débilmente) estacionario** si:
 ▶ $E(y_t) = \mu < \infty$ para todo $t$.  
 ▶ $Cov(y_t, y_{t-j}) = \gamma_j < \infty$ para todo $j, t$.

▶ Estacionariedad ⇒ $Cov(y_t, y_{t-j}) = Cov(y_t, y_{t+j})$, i.e. $\gamma_j = \gamma_{-j}$.

## Algunos procesos estacionarios: Ruido Blanco
El pasado no es informativo sobre el presente. Algo dice igual, el promedio es 0 y la varianza acota el rango de fluctuaciones con probabilidad en un rango.

▶ **Ruido Blanco (RB):** un proceso $\epsilon_t$ sigue un proceso RB si:
 ▶ $E(\epsilon_t) = 0$ para todo $t$.  
 ▶ $V(\epsilon_t) = \sigma^2 < \infty$ para todo $t$.  
 ▶ $Cov(\epsilon_t, \epsilon_{t-j}) = 0$, para $j > 0$ y para todo $t$.

▶ Se sigue que un RB es estacionario, con $\mu = 0$, $\gamma_0 = \sigma^2$ y $\gamma_j = 0$ para $j > 0$. Escribimos $\epsilon_t \sim RB(0, \sigma^2)$.

▶ Un caso particular: si $\epsilon_t$ se extrae de una distribución normal independiente e idénticamente distribuida (i.i.d.); i.e. $\epsilon_t \sim N(0, \sigma^2)$ para todo $t$.

## Algunos procesos estacionarios: AR

▶ **Proceso auto-regresivo de orden 1, AR(1):**
$$y_t = c + \phi y_{t-1} + \epsilon_t$$
con $\epsilon_t \sim RB(0, \sigma^2)$, y $|c|, \sigma < \infty$.

▶ Veremos que este proceso es estacionario si $|\phi| < 1$.

▶ Note que si $y_t = c + \phi y_{t-1} + \epsilon_t$ para todo $t$, también se cumple que $y_{t-1} = c + \phi y_{t-2} + \epsilon_{t-1}$. Así,
$$y_t = c + \phi c + \phi^2 y_{t-2} + \epsilon_t + \phi \epsilon_{t-1}.$$

▶ Haciendo esto hasta un período $n$,
$$y_t = c + \phi c + ... + \phi^n c + \phi^{n+1} y_{t-(n+1)} + \epsilon_t + \phi \epsilon_{t-1} + ... + \phi^n \epsilon_{t-n}$$
$$= \mu \sum_{j=0}^n \phi^j + \phi^{n+1} y_{t-(n+1)} + \sum_{j=0}^n \phi^j \epsilon_{t-j}.$$

▶ Tomando el límite con $n \to \infty$, si $|\phi| < 1$ tenemos
 ▶ $\lim_{n\to\infty} \sum_{j=0}^n \phi^j = \sum_{j=0}^\infty \phi^j = \frac{1}{1-\phi}$.  
 ▶ $\lim_{n\to\infty} \phi^{n+1} y_{t-(n+1)} = 0$.  
 ▶ $\lim_{n\to\infty} \sum_{j=0}^n \phi^j \epsilon_{t-j} = \sum_{j=0}^\infty \phi^j \epsilon_{t-j}$ está bien definido (i.e. no es $\infty$).

▶ Entonces, si $|\phi| < 1$,
$$y_t = \frac{c}{1-\phi} + \sum_{j=0}^\infty \phi^j \epsilon_{t-j}.$$

Esto se llama la representación MA($\infty$) de un proceso AR(1) (más adelante).

▶ Usando esto, podemos calcular,
 • $E(y_t) = \frac{c}{1-\phi} + \sum_{j=0}^\infty \phi^j E(\epsilon_{t-j}) = \frac{c}{1-\phi}$.  
 • $V(y_t) = E[(y_t - E(y_t))^2] = \sum_{j=0}^\infty (\phi^2)^j E(\epsilon_{t-j}^2) = \frac{\sigma^2}{1-\phi^2}$.  
 • $Cov(y_t, y_{t-s}) = E[(y_t - E(y_t))(y_{t-s} - E(y_t))] = \phi^s \frac{\sigma^2}{1-\phi^2}$.

## Algunos procesos estacionarios: AR y MA

▶ **Proceso auto-regresivo de orden p, AR(p):**
$$y_t = c + \phi_1 y_{t-1} + ... + \phi_p y_{t-p} + \epsilon_t$$
con $\epsilon_t \sim RB(0, \sigma^2)$, y $|c|, \sigma < \infty$. Las condiciones para estacionariedad son más complicadas.

▶ **Media móvil de orden 1, MA(1):**
$$y_t = c + \epsilon_t + \theta \epsilon_{t-1}$$
con $\epsilon_t \sim RB(0, \sigma^2)$ para todo $t$, y $|c|, |\theta|, \sigma < \infty$. Propiedades:
- $E(y_t) = E(c + \epsilon_t + \theta \epsilon_{t-1}) = \mu + E(\epsilon_t) + \theta E(\epsilon_{t-1}) = c$.  
- $V(y_t) = E[(y_t - c)^2] = E[(\epsilon_t + \theta \epsilon_{t-1})^2] = E(\epsilon_t^2 + \theta^2 \epsilon_{t-1}^2 + \epsilon_t \theta \epsilon_{t-1}) = (1 + \theta^2)\sigma^2$.  
- $Cov(y_t, y_{t-1}) = E[(y_t - c)(y_{t-1} - c)] = E[(\epsilon_t + \theta \epsilon_{t-1})(\epsilon_{t-1} + \theta \epsilon_{t-2})] = \theta \sigma^2$  
- Para $j > 1$, $Cov(y_t, y_{t-j}) = 0$

▶ El proceso MA(1) es siempre estacionario.

## Algunos procesos estacionarios: MA

▶ **Media móvil de orden q, MA(q):**
$$y_t = c + \epsilon_t + \theta_1 \epsilon_{t-1} + ... + \theta_q \epsilon_{t-q}$$
con $\epsilon_t \sim RB(0, \sigma^2)$ para todo $t$, y $|c|, |\theta_1|, ..., |\theta_q|, \sigma < \infty$. MA(q) es estacionario si $|\theta_j| < \infty$ para $j = 1, ..., q$.

▶ **Media móvil de orden $\infty$, MA($\infty$):**
$$y_t = c + \sum_{j=0}^\infty \theta_j \epsilon_{t-j}$$
con $\epsilon_t \sim RB(0, \sigma^2)$ para todo $t$ y $\theta_0 = 1$.

▶ Se puede demostrar que MA($\infty$) es estacionario si $\sum_{j=0}^\infty |\theta_j| < \infty$.

## Algunos procesos estacionarios: ARMA y Teorema de Representación de Wold

▶ **Generalización: ARMA(p,q):**
$$y_t = c + \phi_1 y_{t-1} + ... + \phi_p y_{t-p} + \epsilon_t + \theta_1 \epsilon_{t-1} + ... + \theta_q \epsilon_{t-q}$$
con $\epsilon_t \sim RB(0, \sigma^2)$.

▶ Se puede demostrar que el proceso es estacionario siempre y cuando la parte AR lo sea.

▶ **Teorema de Representación de Wold:** Todo proceso estacionario tiene una representación $MA(\infty)$.

▶ En particular, todo proceso $ARMA(p, q)$ estacionario tiene una representación $MA(\infty)$.

▶ Esto implica que, aunque no conozcamos el verdadero proceso generador de datos, si la serie es estacionaria tiene una representación $MA(\infty)$. Entonces, como los modelos ARMA también tienen una representación $MA(\infty)$, podemos intentar aproximar el verdadero proceso $MA(\infty)$ (que no puede ser estimado porque tiene $\infty$ parámetros) con un proceso ARMA que sí puede ser estimado.

## Simplificando la notación: Operador de rezago

▶ El **operador de rezago**, $L$, aplicado a un proceso $\{y_t\}$:
 ▶ $Ly_t \equiv y_{t-1}$  
 ▶ $L^2 y_t \equiv L(Ly_t) = L(y_{t-1}) = y_{t-2}$ ...  
 ▶ $L^k y_t = y_{t-k}$

▶ **Propiedades:**
 ▶ $L(\alpha x_t) = \alpha L x_t$  
 ▶ $L(x_t + y_t) = L x_t + L y_t$

▶ **Polinomio en el operador de rezago:** $\alpha_k(L) \equiv \alpha_0 + \alpha_1 L + \alpha_2 L^2 + ... + \alpha_k L^k$.

▶ Entonces: $\alpha_k(L)y_t = \alpha_0 y_t + \alpha_1 y_{t-1} + \alpha_2 y_{t-2} + ... + \alpha_k y_{t-k}$.

▶ Considere un proceso ARMA(p, q):
$$y_t = \phi_1 y_{t-1} + ... + \phi_p y_{t-p} + \varepsilon_t + \theta_1 \varepsilon_{t-1} + ... + \theta_q \varepsilon_{t-q}$$

▶ Usando $L$ podemos escribir:
$$(1 - \phi_1 L - ... - \phi_p L^p)y_t = (1 + \theta_1 L + ... + \theta_q L^q)\varepsilon_t$$
o,
$$\Phi_p(L)y_t = \Theta_q(L)\varepsilon_t$$
donde $\Phi_p(L) \equiv 1 - \phi_1 L - ... - \phi_p L^p$ y $\Theta_q(L) \equiv 1 + \theta_1 L + ... + \theta_q L^q$.

# Regresiones con variables estacionarias

▶ **Modelo lineal simple:**
$$y_t = \beta z_t + u_t$$
$z_t$ puede ser un vector. Asuma que $y_t, z_t, u_t$ son estacionarios.

▶ **Supuesto clave del modelo lineal: exogeneidad.** Alternativas:
 ▶ Contemporánea: $E(u_t|z_t) = 0$.  
 ▶ Débil: $E(u_t|z_t, z_{t-1}, z_{t-2}, ...) = 0$.  
 ▶ Fuerte: $E(u_t|..., z_{t+2}, z_{t+1}, z_t, z_{t-1}, z_{t-2}, ...) = 0$.

▶ $\beta$ puede ser estimado consistentemente por MCO bajo cualquiera de estos tres (la consistencia es más complicada en series temporales, más adelante).

▶ Si podemos justificar que uno de estos se cumple depende de la aplicación específica.

▶ **Modelo de rezago distribuido (RD):**
$$y_t = \alpha + \beta_0 x_t + \beta_1 x_{t-1} + ... + \beta_q x_{t-q} + u_t.$$
La consistencia requiere exogeneidad débil o fuerte de $x_t$.  
Note que podemos escribir este modelo como en la especificación con $z_t$ arriba.

▶ La varianza asintótica de MCO depende de los segundos momentos de $u_t$.

▶ Recuerde los supuestos clásicos (Gauss-Markov):
 ▶ Homocedasticidad: $Var(u_t) = \sigma^2$ para todo $t$.  
 ▶ No correlación serial: $Cov(u_t, u_\tau) = 0$ para $t \neq \tau$.

▶ La persistencia en series temporales presenta un desafío para el segundo supuesto. Si los residuos presentan correlación serial, la varianza de MCO estará sesgada, llevando a sobre-rechazo de la hipótesis nula (ej. en la prueba para $H_0 : \beta = 0$ usando el estadístico t $\hat{\beta}/\sqrt{\widehat{Var}(\hat{\beta})}$, si $Var(\hat{\beta})$ es artificialmente grande el estadístico t es pequeño, llevando a aceptación incluso si la nula es falsa).

▶ La consistencia no depende de estos supuestos.

▶ Para arreglar este problema, el estimador de varianza HAC o Newey-West, que controla por la posibilidad de autocorrelación y heterocedasticidad en el residuo.

▶ **Generalización: modelo ARD**
$$y_t = \alpha + \gamma_1 y_{t-1} + \gamma_2 y_{t-2} + ... + \gamma_p y_{t-p} + \beta_0 x_t + \beta_1 x_{t-1} + ... + \beta_q x_{t-q} + u_t.$$

▶ Dinámicas más ricas que el modelo RD.

▶ La auto-correlación en $u_t$ puede causar problemas de consistencia (elegir $p$ es bastante relevante).

# Raíces Unitarias y Co-integración

▶ Recuerde: Un proceso es estacionario si
 ▶ $E(y_t) = \mu < \infty$ para todo $t$.  
 ▶ $Cov(y_t, y_{t-j}) = \gamma_j < \infty$ para todo $j, t$.

Entonces, $y_t$ es no-estacionario si al menos uno de ellos no se cumple.

▶ Diferentes procesos no-estacionarios difieren dependiendo de la fuente de no-estacionariedad (ej. media, varianza, etc.). Describimos 3 tipos:
 ▶ Tendencia determinística.  
 ▶ Caminata aleatoria.  
 ▶ Caminata aleatoria con deriva.

### Tendencia determinística (TD)
$y_t = d_t + \epsilon_t$,  
donde $d_t$ es una función conocida del tiempo (ej. $d_t = \alpha + \beta t$) y $\epsilon_t \sim RB(0, \sigma^2)$. Así,
 ▶ $E(y_t) = d_t$.  
 ▶ $V(y_t) = V(\epsilon_t) = \sigma^2$.

La fuente de no-estacionariedad es la media.

### Random Walk (RW)
Varianza no acotada
$y_t = y_{t-1} + \epsilon_t$, donde $\epsilon_t \sim RB(0, \sigma^2)$.  
Podemos iterar hacia atrás para obtener,
$$y_t = y_{t-1} + \epsilon_t = y_{t-2} + \epsilon_{t-1} + \epsilon_t = ... = y_0 + \sum_{j=1}^t \epsilon_j$$

Entonces
 ▶ $E(y_t) = y_0$.  
 ▶ $V(y_t) = tV(\epsilon_t) = t\sigma^2$.

La fuente de no-estacionariedad es la varianza.

### Random Walk with Drift (RWD)
$y_t = \alpha + y_{t-1} + \epsilon_t$, donde $\epsilon_t \sim RB(0, \sigma^2)$. Iterando hacia atrás,
$$y_t = \alpha + y_{t-1} + \epsilon_t = \alpha + \alpha + y_{t-2} + \epsilon_{t-1} + \epsilon_t = ... = \alpha t + y_0 + \sum_{j=1}^t \epsilon_j$$

Entonces
 ▶ $E(y_t) = y_0 + \alpha t$.  
 ▶ $V(y_t) = tV(\epsilon_t) = t\sigma^2$.

Las fuentes de no-estacionariedad son tanto la media como la varianza. Una CAD es una TD lineal más una CA.

▶ **Generalización:** Reemplazando $\epsilon_t \sim RB(0, \sigma^2)$ con $u_t$ siendo estacionario con media cero.

▶ **Prueba de raíz unitaria:** En la siguiente regresión
$$y_t = \alpha + \beta t + \phi y_{t-1} + u_t,$$
probar
$$H_0 : \phi = 1 \text{ vs. } H_A : |\phi| < 1$$

El estadístico t tiene una distribución no estándar. DF, ADF, DF-GLS, ...

▶ **Orden de integración:** $y_t$ es $I(d)$ si $y_t - y_{t-d}$ es estacionario.

▶ **Co-integración:** dos series $I(d)$ $y_t, x_t$ están co-integradas si hay una combinación lineal entre ellas que es $I(0)$.

▶ **Interpretación:** Tendencia estocástica común (co-integración $\nRightarrow$ Causalidad).

▶ Las regresiones en niveles con variables no-estacionarias tienen sentido solo si están co-integradas. El problema no es la consistencia, es la varianza.

▶ **Prueba simple de co-integración (Engle-Granger):** Ejecutar una regresión en niveles, realizar una prueba de raíz unitaria en los residuos.

▶ Discutiremos más sobre co-integración en el contexto de vectores de variables (VEC)

# Comentarios sobre Asintóticas en Series Temporales

▶ En Econometría Avanzada estudiaste extensivamente las propiedades asintóticas de MCO, VI, MV. Resultados principales: LGN, TCL, Método Delta.

▶ Supuesto clave en corte transversal: muestras i.i.d.

▶ Esto no se cumple en series temporales. Un tratamiento completo está más allá del alcance de este curso (si le interesa, vea Hamilton, 1994, Ch. 7, 16, 17).

▶ **Algunos comentarios:**
▶ Estacionariedad débil: la mayoría de los resultados se mantienen. La varianza de largo plazo es más complicada (recuerde HAC).  
▶ Raíces unitarias: super-consistencia, la distribución asintótica no es normal (DF), local a la unidad.