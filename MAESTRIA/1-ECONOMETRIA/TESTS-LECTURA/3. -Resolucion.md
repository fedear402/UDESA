# Anatomía de MCO: sesgo y varianza
## Test de Lectura
1.**¿Por qué en el slide 4 es lo mismo usar $Y_i$ que $Y_i^*$?** Idempotencia y simetría de $P_{X}$ **2.¿Cuál es el problema de decir que 'la multicolinealidad se arregla aumentando n'?** Si tuvieras mas observaciones ya las hubieras agregado **3.En el modelo básico $Y_i = \alpha + \beta X_i + u_i$, $i = 1, \ldots, n$, ¿qué significa multicolinealidad exacta? ¿Cómo sería un dibujo de los datos $(X_i, Y_i)$ en ese caso?** Escalados **4.Verdadero o falso: a) La omisión de variables relevantes sesga al estimador de MCO.** Solo si **b) De existir, el sesgo se achica con el tamaño de la muestra** No, sesgo es para muestra finita **c) Si una variable omitida tiene correlación positiva con la variable explicada, el sesgo por omitirla es también positivo.** Depende de como correlaciona con la incluida.  1.**De acuerdo al TFWL $\hat{\beta}_j = \frac{\sum X_{ji}^* Y_i}{\sum X_{ji}^{*2}}, \quad j = 1, \ldots, K$ Tomando como punto de partida la fórmula anterior, mostrá que $\hat{\beta}_j$ es insesgado. Una vez que lo hiciste, fijate que la prueba vale para todo $j = 1, \ldots, K$. Entonces: habremos probado que $\hat{\beta}$ es insesgado.... ¡sin usar matrices!**   $\frac{\sum_{i} ^{n} X_{ji}^* \left[  \beta _{j}X_{ji}+ \sum_{k\neq j}\beta _{k}X _{ki} + \varepsilon _{i} \right] }{\sum X_{ji}^{*2}} \implies \frac{  \left[  \sum_{i} ^{n}\beta _{j}X_{ji}X_{ji}^*+\sum_{i} ^{n} \sum_{k\neq j}\beta _{k}X _{ki} X_{ji}^*+\sum_{i} ^{n} X_{ji}^* \varepsilon _{i} \right] }{\sum X_{ji}^{*2}}$ factor comun los beta y usa -) $\sum _{i} X_{ji}X_{ji}^* = \sum _{i} (X_{ji}^*+X_{ji} - X_{ji}^*)X_{ji}^*$ Y ahi $X_{ji} - X_{ji}^*=X_{ji} ^{P}$ (el valor predicho ie: $e=y-\hat{y}$, $y=\hat{y}-e$).  $\implies \sum _{i} (X_{ji}^{*2}+X_{ji}^{P}X_{ji}^*)$  el termino $X_{ji}^{P}X_{ji}^*$ es el producto interno entre $\hat{y}^{T}e$ que por definicion son ortogonales entonces es 0. Misma logica: $\sum_{i} ^{n} X _{ki} X_{ji}^*$ 

2.Considerá un modelo simple $Y_i = X_i\beta + u_i$ (sin intercepto) en donde valen todos los supuestos clásicos. Pensá que $X_i$ es el ingreso permanente de una persona, y que $\dot{X}_i$ es el ingreso corriente. Suponete que $\dot{X}_i = X_i + \omega_i$ es decir, el ingreso corriente es el permanente más un 'error de medición', $\omega_i$, que supondremos que es una variable aleatoria con esperanza nula, con varianza constante y no correlacionada con $X_i$ 
sisi pni con $u_i$, dado $X_i$. Considerá el siguiente estimador $\tilde{\beta} = \frac{\sum Y_i \dot{X}_i}{\sum \dot{X}_i^2}$ Es decir, $\tilde{\beta}$ es el estimador de MCO que regresa $Y_i$ en el ingreso corriente en vez del permanente. Demostrá que este estimador es sesgado. ¿Tenés alguna intuición de por qué aparece este sesgo? (i.e, ¿cuál es la 'variable omitida' que lo causa?)
$$
\tilde{\beta} = \frac{\sum Y_i \dot{X}_i}{\sum \dot{X}_i^2} \implies \tilde{\beta} = \frac{\sum  \dot{X}_i (X_{i}\beta+u_{i}) }{\sum \dot{X}_i^2} \implies \tilde{\beta} = \frac{\sum  \dot{X}_i ( \dot{X}_i\beta-\omega _{i}\beta+u_{i})}{\sum \dot{X}_i^2}
$$
$$
\tilde{\beta} = \frac{\sum  \dot{X}_i ( \dot{X}_i\beta-\omega _{i}\beta+u_{i})}{\sum \dot{X}_i^2} 
$$

