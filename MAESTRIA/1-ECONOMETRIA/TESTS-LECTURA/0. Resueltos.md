# 1. Modelo Lineal. Mínimos Cuadrados
**1.¿Por qué decimos que en el modelo lineal hay n ecuaciones con $n + K$ incógnitas?** Modelo lineal : $Y_{i}=\beta_{1}+\beta_{2}X_{i}+ \dots + \beta _{K}X_{i} +u_{i}$. Para cada observación $i \dots n$. $X$ e $Y$ son valores conocidos para todo $i$, los $n$ valores de $u$ y las $K$ variables no se conocen, **2. ¿Qué hay de raro en hablar del 'modelo de mínimos cuadrados'?** Es el método de minimios cuadrados y el modelo es de regresión lineal **3. ¿En qué sentido decimos que es relevante estimar más que conocer el valor de los coeficientes $\beta$?** Como el sistema de ecuaciones tiene $K$ + $n$ incógnitas pero $n$ ecuaciones está sobredeterminado y no tiene una unica solucion. Se bisca estimar porque se busca de las respuestas que lo solucionan la que aproxime mejor el valor problacional modelado. **4. Si el objetivo es minimizar el tamaño del error, ¿por qué no elegir un estimador que haga que $e = 0$?** No tiene sentido porque se te cancelan los signos, cuadrados minimiza las distancias. La suma de $e$ termina siendo 0 por consecuencia. DERIVACION MCO: $\min_{ \hat{\beta}} \sum _{i=1}^{n} e_{i}^{2}$ $\implies$ $\min _{\hat{\beta}} e'e$ $\implies$ $\min _{\hat{\beta}} (Y-X\hat{\beta})'(Y-X\hat{\beta})$ $\implies$ $\min _{\hat{\beta}}Y'Y- \hat{\beta}'X'Y-Y'X\hat{\beta}+\hat{\beta}'X'X\hat{\beta}$ $\implies$ $\min _{\hat{\beta}} Y'Y-2 \hat{\beta}'X'Y+\hat{\beta}'X'X\hat{\beta}$  ¿por qué son iguales $\hat{\beta}'X'Y=Y'X\hat{\beta}$? Son escalares y uno es la transpuesta del otro. $\frac{ \partial e'e }{ \partial \hat{\beta} } = -2X'Y + (X'X + (X'X)')\hat{\beta} = 0$ $\implies$ $\frac{ \partial e'e }{ \partial \hat{\beta} } = -2X'Y + 2X'X\hat{\beta} = 0$.=$\implies$ $2X'X\hat{\beta} = 2X'Y$ $\implies$ $\hat{\beta} = (X'X)^{-1}X'Y$. Asumiendo que existe la inversa $(X'X)^{-1}$. ¿Qué quiere decir ese supuesto?  No hay multicolinealidad perfecta: $\rho(X) = K \implies \rho(X'X) = K$.  **5.Verdadero o Falso (V o F): si $\rho(X) = K$, entonces X es invertible.** F. $X^{n \times K}$ no es invertible porque $K<n$: no es cuadrada. **6. Si Y son salarios y X es el nivel de educación en años de n personas en una muestra de corte transversal, intuitivamente, en este caso, ¿qué significa el supuesto de no correlación serial?** Para el mismo nivel de educ los determinantes del salario de $i$ no correlacionan con los de $j$. 

 7. ¿Cuál pensás que es el único supuesto que es prácticamente imposible levantar?

 8. Si en una clase hay 100 alumnos que se sientan en 5 filas. La forma directa de calcular el promedio es sumar las edades de todos y dividir por 100. De acuerdo a la ley de esperanzas iteradas, ¿cuál es el método indirecto de calcular el promedio, explotando la forma en la que se han sentado los alumnos?

 9. Considerá las cuatro propiedades que establecimos y probamos (linealidad, insesgadez, varianza y gauss/markov). Armá una tabla de doble entrada. En las filas poné los supuestos clásicos y en las columnas los cuatro resultados. Marcá con una 'X' qué supuestos se usan en cada demostración (insesgadez, fórmula de la varianza, Gauss/Markov).

 10. Intuitivamente, ¿cuál te parece que es 'el' supuesto del cual se deduce que el estimador MCO es insesgado?

 11. En la demostración de Gauss/Markov mostramos que $AZX = I$. ¿Por qué en general no es posible deducir que $A = X^{-1}$? ¿En qué único caso sí sería posible?

 12. ¿Por qué si mostramos que $Cov(\hat{\beta}, \hat{\gamma}) = 0$, de ahí se deduce fácilmente la demostración del resultado fundamental de Gauss/Markov?

 13. ¿Por qué si u es normal $\hat{\beta}$ es normal?

 14. ¿Cuál el rol de la linealidad del modelo y de la del método en el resultado anterior?

 15. Para pensar: en el slide 28 hay una salida de regresión. A partir de estos resultados, ¿cómo se 've' que el estimador es insesgado o no? ¿Y que tiene varianza mínima?

 16. En la página les dejé una nota titulada 'OLS without derivatives'. Es una 'derivación' de MCO sin usar derivadas. Leé la nota con detalle y compará con la forma en la que obtuviste el estimador de MCO en un curso básico. Pregunta que dejo picando: ¿por qué puse 'derivación' entre comillas?

 17. En la página les dejé una demostración de que $\rho(X) = K$ implica que $(X'X)$ es invertible. Leela con detalle (la demostración está en la primera página, el resto, no es relevante para este curso, si para el de Machine Learning)

 18. Nota de color: la invención del método de MCO fue durante muchísimos años disputada entre Gauss y Legendre. Googlea en relación a este tema. Una vez que termines este curso te recomiendo la siguiente lectura: Stigler, Stephen M. (1981). 'Gauss and the Invention of Least Squares'. Annals of Statistics, 9 (3): 465–474.

 19. Nota de color II: hace poco Bruce Hansen causó un pequeño terremoto en la profesión, al presentar un trabajo en donde parece que la 'L' sobra en la parte BLUE (Best Linear and Unbiased Estimator) del teorema de Gauss/Markov. Buscá en twitter el jugoso intercambio entre Jeffrey Wooldridge y Bruce Hansen al respecto. Es técnico, no es necesario que lo hagas, pero es interesante.
Ejercicios

1.Demostrá que si $\rho(X) = K$, entonces $n \geq K$.

2.Para el caso en que $n = K$, obtené cuánto valen $\hat{\beta}$, $\hat{Y}$, e y $\sum e_i^2$.

3.Consideremos el modelo lineal $Y = X\beta + u$ en donde valen todos los supuestos clásicos. Sea H una matriz $n \times n$ invertible y no aleatoria. Consideremos el siguiente estimador $\hat{\beta}^* = (X^{*'} X^*)^{-1} X^{*'} Y^*$. En donde $X^* \equiv HX$ y $Y^* \equiv HY$. Demostrá que el estimador es insesgado y obtené su varianza. ¿Por qué este estimador no puede ser más eficiente que el de mínimos cuadrados?


